\subsection{Multithreading / Multiprocessing}
\label{sec:org49c893c}

Typically, programs execute individual instructions sequentially, which is often described as synchronous code execution. With modern hardware and core counts, this is inefficient and leads to poor resource utilisation. There are two common situations where \emph{asynchronous} or \emph{parallel} code is valuable. The differences are explained in the examples below:\\

\begin{enumerate}
\item The completion of an external process must be awaited, such as a disk read/write operation or the return from a REST API. In this case, the CPU is left idle, waiting for an external event to occur. This idle time could be spent initiating another operation instead of spinning and wasting processor cycles. Very abstract languages like python and javascript perform \emph{asynchronous threading}, where execution continues after initiating the external task. The program still has just one thread of execution and is serviced by single CPU, although this thread can continue past blockages and long-running function calls. This behaviour is sometimes called 'non-blocking'.\\

\item An expensive, time consuming computation needs processor time. In this case, the processing is not offloaded from the processor and threading offers no performance gains. To improve the performance of such a program, computation must be distributed across many processor cores which can do processing at the same time, in \emph{parallel}.\\
\end{enumerate}

Python uses the term ``multithreaded'' to refer to the former and ``multiprocessing'' to the latter execution model. C, where I will implement parallelism, uses the term ``thread'' to refer to code which runs on a separate processor, as in the second example. Consequently, this is how I will use the term from now on, as the concept of parallelism is introduced.\\

\subsubsection{Python Example}
\label{sec:org9a151f9}

Multiple concurrent execution is a difficult topic to understand and implement properly. Much like recursion, the programmer needs to consider how paths are taken at different stage, although threading introduces further complexities, principally the uncertainty of the thread execution order. After creating a thread or process, the processor time it receives is dictated by the system scheduler and there is no way of telling which threads run first or when other threads are interrupted. I have prepared two very simple examples of the scenarios listed above. Both techniques are used in places throughout my project.\\
The first situation is resolved with simple \emph{threading} (in python speak). The delay is caused by some external process, simulated by the python \texttt{sleep()} function. The program need to call this long-running function 20 times and so the program will take at least 20s to execute.\\

\begin{minted}[ frame=single, framesep=8mm, rulecolor=RuleGray, baselinestretch=1.3, fontsize=\small, breaklines, fontfamily=tt]{python}

import time

def delay():
    time.sleep(1)
    print("process finished")

start = time.time()

for i in range(20):
    delay()

print(time.time() - start)

\end{minted}

In a case like this, it would be a good idea to use python \texttt{threading} to start these threads in the background and service them with processor time only when required.\\

\begin{minted}[ frame=single, framesep=8mm, rulecolor=RuleGray, baselinestretch=1.3, fontsize=\small, breaklines, fontfamily=tt]{python}

import time
from threading import Thread

def delay():
    time.sleep(1)
    print("process finished")

start = time.time()
threads = []

for i in range(20):
    thread = Thread(target=delay)
    threads.append(thread)
    thread.start()

for thread in threads:
    thread.join()

print(time.time() - start)

\end{minted}

While this is a simple introduction to threading, many of the same principles appear in the more complicated examples later on. The function to execute remains the same, the difference being the call site. Here, 20 threads are created, set to work and recorded in a list. The main thread is free to move on and execute other instructions. It is common that the remaining code requires that the threads have run and possibly returned a value (seen later on). In such a case, the program can \texttt{join()} active threads and ensure that they finish before execution resumes. The result of these improvements is a program which can be completed in just 1s or thereabouts, as opposed to the 20s it took before.\\

In the other scenario presented, the problem cannot be solved without doing more processing simultaneously. This requires a machine capable of multi-tasking, one with multiple cores. Notably, the target operation now does some processing, instead of waiting idly. In actual fact this function does not take long to execute at all; it is purely representative of the underlying concept.\\

\begin{minted}[ frame=single, framesep=8mm, rulecolor=RuleGray, baselinestretch=1.3, fontsize=\small, breaklines, fontfamily=tt]{python}
import time

def operation(input):
    print(input * input)

start = time.time()
data = [3,4,5,6,7,8,9]

for input in data:
    operation(input)

print(time.time() - start)

\end{minted}

The script calls the operation on pieces of data from a list one by one. This can be made more efficient by running the operations simultaneously.\\

\begin{minted}[ frame=single, framesep=8mm, rulecolor=RuleGray, baselinestretch=1.3, fontsize=\small, breaklines, fontfamily=tt]{python}

import time
from multiprocessing import Process

def operation(input):
    print(input * input)

start = time.time()
processes = []
data = [3,4,5,6,7,8,9]

for input in data:
    process = Process(target=operation, args=(input,))
    processes.append(process)
    process.start()

for process in processes:
    process.join()

print(time.time() - start)

\end{minted}

Syntactically, this API is no different from the one used for threading. Processes can achieve the same results as threads in the first example, but the opposite is not true. Threading will have no performance advantage in the second example. Creating threads and processes does have penalties, such as the time taken to create the threads and the memory allocated to each one.\\

\subsubsection{Asynchronous Javascript}
\label{sec:orgaf3e181}

JavaScript code in the NodeJs runtime behaves asynchronously. During IO bound operations, the main thread will pass function calls and continue to process subsequent instructions before a response is received for the first operation. In order to guarantee the sequential execution of code and make sure all the necessary program data is available, a combination of \textbf{callbacks} and \textbf{promises} can be used. Of these two techniques, the latter is more professional and is generally preferred.\\

Despite having innumerable drawbacks, a callback is easy to implement. If it is essential that one function has to run after the other, it is simply passed to the initial function. After the first function has done its work, it can call the function that it was passed.\\

\begin{minted}[ frame=single, framesep=8mm, rulecolor=RuleGray, baselinestretch=1.3, fontsize=\small, breaklines, fontfamily=tt]{javascript}

function one(two){
    two("function one completed")
}

one((message) => {
    console.log(message);
});

\end{minted}

Evidenced in the example above, the callback function is often defined while setting up the call to the first function. This syntax looks quite elegant when there is a single callback, although programs grow in complexity very rapidly, leading to something colloquially know as the \emph{callback pyramid of doom}. Problems are know to appear when error handling is involved. It is difficult to propagate and track errors through the \emph{pyramid} created by many levels of indentation.\\

Designed to address the awkwardness of callbacks and many of their limitations, is the \emph{Promise}. A single promise maintains the state of an asynchronous operation. The state may be any of these three:\\
\begin{itemize}[noitemsep, topsep=0pt]
    \item Pending
    \item Fulfilled
    \item Rejected
\end{itemize}
~\newline
If a promise is said to be in a \emph{Settled} or \emph{Resolved} state it is not pending. Promises allow a programmer to change the state of an operation within a promise and define behaviour for the success or failure of a promise. The name comes about as the Promise is a type of negotiation between the call site and the function.\\


\begin{minted}[ frame=single, framesep=8mm, rulecolor=RuleGray, baselinestretch=1.3, fontsize=\small, breaklines, fontfamily=tt]{javascript}

function one(condition) {
    return new Promise((resolve, reject) => {
        if (condition == true) {
            resolve("success")
        } else {
            reject("failure")
        }
    });
}

one(true).then((message) => {
    // success condition
}).catch((message) => {
    // error condition
});

\end{minted}

A promise accepts the arguments \texttt{resolve} and \texttt{reject}. Upon success the promise will call \texttt{resolve}, passing any data that is to be returned. Access to the return value is permitted with the \texttt{.then()} operation on the returned promise. Should the function fail, the promise can \texttt{reject} with any arguments. This event will trigger the \texttt{.catch()} operation for the promise, so the program can handle the exception.\\

This technique makes error handling very easy, although much of the program code remains nested. The \texttt{async/await} syntax can be employed to simplify code structure, at the expense of built in error handling, which is easily handled with a \texttt{try=/=except} block.\\

\begin{minted}[ frame=single, framesep=8mm, rulecolor=RuleGray, baselinestretch=1.3, fontsize=\small, breaklines, fontfamily=tt]{javascript}

function test(condition) {
    return new Promise((resolve, reject) => {
        if (condition == true) {
            resolve("success")
        } else {
            reject("failure")
        }
    });
}

async function main() {
    try {
        let message = await test(true)
        console.log(message)
    } catch (message) {
        console.log(message)
    } finally {
        console.log("main has completed");
    }

}

main();

\end{minted}

\subsubsection{POSIX / C Threading}
\label{sec:org7cbb592}

In and ideal scenario, the duration of a particular subroutine is inversely proportional to the number of threads that work on it, although this isn't always the case. Multiprocessing introduces a range of potentially dangerous problems and a number of advanced techniques have been developed to solve them, many are the work of Edsger W. Dijkstra. The following sections detail many common threading problems.\\

Threads occupy the same address space as the main thread, unlike new \emph{processes} which have their own. Each thread has its own region of stack memory in the address space of the program, illustrated below. The heap belongs to the process and is shared between all threads.\\

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=0.4\textheight,keepaspectratio,frame]{./images/address_space_threads.png}
\caption{Multiple threads in a single address space}
\end{figure}

Parallelism is generally used to work on large quantities of data, which typically resides on the heap. Similarly, any data that is returned from a thread must be heap allocated, as the stack is destroyed when the thread exits.\\

Threads are created with a target routine/function and given a list of arguments. The implementation of the thread API is platform dependent, I will use the Linux implementation of POSIX threads defined in \texttt{<pthreads.h>}.\\

\subsubsection{Critical Section}
\label{sec:org83fe778}

As discussed above, individual threads can and often do \emph{share} resources. A \emph{critical section} is an area of code where a thread attempts to access one of these resources which is shared between a number of concurrent threads. The danger of these critical sections is inducing a \emph{race condition} between two or more threads competing to use a resource. If data is shared, access to it must be carefully managed to avoid compromising its integrity. Data structures and algorithms which are designed to cope with multiple threads are described as \emph{thread-safe}.\\

Used to prevent these harmful race conditions are a set of low-level mechanisms called \emph{synchronisation primitives}. Examples include \emph{mutex locks}, \emph{condition variables} and \emph{semaphores}. These are often used to deliver \emph{mutual-exclusion} or \emph{atomicity}, the guarantee that a series of instructions are executed in their entirety without interuption. Both of these concepts will be prototyped in this section before being implemented in the codebase of the application.\\

Frustratingly, the improper use of these synchronisation techniques can lead to equally sinister problems, ranging from delayed code execution to \emph{deadlock} and \emph{starvation}.\\

\subsubsection{Race Condition}
\label{sec:org8d8eed0}

This pseudocode demonstrates a typical race condition. The program is designed to increment the \texttt{shared\_counter} variable by 2 million. It creates two threads, which execute simultaneously.\\

\begin{minted}[ frame=single, framesep=8mm, rulecolor=RuleGray, baselinestretch=1.3, fontsize=\small, breaklines, fontfamily=tt]{text}

GLOBAL integer shared_counter <- 0

SUB inc_counter

    FOR (local_counter <- 0 TO 1000000 STEP 1)
        GLOBAL shared_counter += 1
    END FOR

END_SUB

SUB main

    OUTPUT shared_counter

    thread t1
    thread t2

    thread_start(t1, inc_counter, NONE)
    thread_start(t2, inc_counter, NONE)

    thread_join(t1, NONE)
    thread_join(t2, NONE)

    OUTPUT shared_counter

END  SUB

\end{minted}

\texttt{thread\_start} represents the typical approach to creating a thread, across languages and systems. The function requires a thread, target function and list of arguments - in this case there are none. The function \texttt{thread\_join} is used to wait for a thread to exit. This might be done if the return value is needed, or the program would otherwise exit.\\

There is a problem with the algorithm shown above. Running the program should return \texttt{2000000}, although it always produces a slightly different, smaller number. This situation is caused by the race condition in the code. The offending line is \texttt{GLOBAL shared\_counter +=1}. While this appears to be one operation, it is in fact three. The assembly language below illustrates this.\\

\begin{minted}[ frame=single, framesep=8mm, rulecolor=RuleGray, baselinestretch=1.3, fontsize=\small, breaklines, fontfamily=tt]{text}

LDA rax MEM_ADDR
ADD rax, 1
STO MEM_ADDR, rax

\end{minted}

Each thread in a program has its own state, which is managed by the operating system. During execution, it is possible that a thread is interrupted and its operation is suspended. If this occurs while the thread is in the critical region, a synchronisation error can occur. Below is a demonstration of the scenario that could arise in the above program. Thread \texttt{t1} is interupted while it is in the critical region, leaving \texttt{t2} free to access the shared resource. When execution of \texttt{t1} resumes, it writes over the updated value.\\

\begin{longtable}{ |p{0.3\textwidth}|p{0.3\textwidth}|p{0.3\textwidth}| }
\hline
&&\\*[-0.9em]

\textbf{t1} & \textbf{t2} & \textbf{shared{\_}counter} \\*
&&\\*[-0.9em]
\hline
\endhead
&&\\*

read value from memory && \multicolumn{1}{r|}{37}\\*
increment value locally && \multicolumn{1}{r|}{37}\\*
interrupt! && \multicolumn{1}{r|}{37}\\*
& read value from memory & \multicolumn{1}{r|}{37}\\*
& increment value locally & \multicolumn{1}{r|}{37}\\*
& write value to memory & \multicolumn{1}{r|}{38}\\*
thread resumed && \multicolumn{1}{r|}{38}\\*
write value to memory && \multicolumn{1}{r|}{38!}\\*
&&\\*

\hline
\end{longtable}

In this example, the value of \texttt{shared\_counter} is only updated once, despite the fact that two threads have incremented its value.\\

\subsubsection{Atomicity}
\label{sec:org5b791b6}

The trace table below illustrates the desired behaviour of the two threads.\\

\begin{longtable}{ |p{0.3\textwidth}|p{0.3\textwidth}|p{0.3\textwidth}| }
\hline
&&\\*[-0.9em]

\textbf{t1} & \textbf{t2} & \textbf{shared{\_}counter} \\*
&&\\*[-0.9em]
\hline
\endhead
&&\\*

read value from memory && \multicolumn{1}{r|}{37}\\*
increment value locally & waiting... & \multicolumn{1}{r|}{37}\\*
write value to memory &waiting...& \multicolumn{1}{r|}{38}\\*
& read value from memory & \multicolumn{1}{r|}{38}\\*
& increment value locally & \multicolumn{1}{r|}{38}\\*
& write value to memory & \multicolumn{1}{r|}{39}\\*

&&\\*

\hline
\end{longtable}

The first advantage of this approach is the inability of a second thread to enter the critical section, while the critical section is also executed atomically, without interuption. The delay to thread \texttt{t2} is an example of \emph{synchronisation} between threads, which maintains the state of the shared resource. Here is an updated version of the program demonstrated above, with the race condition addressed (new lines prefixed with a \texttt{+} sign):\\

\begin{minted}[ frame=single, framesep=8mm, rulecolor=RuleGray, baselinestretch=1.3, fontsize=\small, breaklines, fontfamily=tt]{text}

GLOBAL integer shared_counter <- 0
+ GLOBAL mutex_lock lock

SUB inc_counter

    FOR (local_counter <- 0 TO 1000000 STEP 1)

+       get_lock(GLOBAL lock)
        GLOBAL shared_counter += 1
+       release_lock(GLOBAL lock)

    END FOR

END_SUB

SUB main

    OUTPUT shared_counter

    thread t1
    thread t2
+   init_lock(lock)

    thread_start(t1, inc_counter, NONE)
    thread_start(t2, inc_counter, NONE)

    thread_join(t1, NONE)
    thread_join(t2, NONE)

    OUTPUT shared_counter

END  SUB

\end{minted}

A \emph{mutex lock} is used to manage access to the critical region that was addressed previously. The term ``mutex'' is short for \emph{mutual-exclusion}, which prevents multiple threads accessing a region of code.\\

The call to \texttt{get\_lock()} does on of two things:\\

\begin{enumerate}
\item obtain the lock if it is not held already, getting exclusive access to the critical section\\
\item spin, waiting for the lock to be free if it is already held\\
\end{enumerate}

Once the thread exits the critical section, the lock is released with a call to \texttt{release\_lock()}. If this is not done at the appropriate time, it is possible that other threads are permanently locked out, or a deadlock might occur.\\

\subsubsection{Semaphores}
\label{sec:org9537665}

The semaphore is another example of a synchronisation primitive. They serve the dual purpose of delivering atomicity, along with the ability to communicate between threads. These two roles are otherwised achieved using a combination of locks and condition variables. This is sometimes referred to as an ordering primitive and lends the programmer greater control of the path threads take through the program.\\

The semaphore is a single number, whose value is shared amongst threads and used to send signals. The value the semaphore is initially set to dictates its behaviour. Once initialised, a semaphore is interacted with through two main methods: \texttt{sem\_post} \& \texttt{sem\_wait}.\\

When \texttt{sem\_post} is encountered, the value of the specified semaphore is \emph{decremented}. If this value is subsequently greater than or equal to \texttt{0}, the thread proceeds into the critical section. If the value after being decremented is less than \texttt{0}, the resource is considered busy and the calling thread waits. The other main function is \texttt{sem\_wait}. Once a thread has completed work in a critical section it sends a signal to other waiting threads by \emph{incrementing} the semaphore value. This may restore the value to \texttt{0} in which case another thread is able to run.\\

Here is an example of a semaphore used to order the execution of two threads, the main program thread and one called \texttt{child\_thread}, demonstrating the proper initialisation of the semaphore. The main thread should start the child thread and then wait until the thread has completed. The main thread is \emph{signalled} to by the child. To achieve this behaviour, the semaphore value is set to \texttt{0}.\\

\begin{minted}[ frame=single, framesep=8mm, rulecolor=RuleGray, baselinestretch=1.3, fontsize=\small, breaklines, fontfamily=tt]{c}
#include <semaphore.h>
#include <stdio.h>
#include <stdlib.h>
#include <pthread.h>

sem_t semaphore;

void * child_func(void *arg) {
    printf("child entering\n");
    printf("child exiting\n");
    sem_post(&semaphore);
    return NULL;
}

int
main(int argc, char *argv[]) {
     sem_init(&semaphore, 0, 0);
     printf("parent entering\n");
     pthread_t child_thread;
     pthread_create(&child_thread, NULL, child_func, NULL);
     sem_wait(&semaphore);
     printf("parent exiting\n");
     return 0;
}
\end{minted}

Consider the outcome of setting the value to \texttt{1}. The main thread would be able to progress past the condition, regardless of the child thread's state. See the trace table below.\\

\begin{longtable}{ |p{0.3\textwidth}|p{0.3\textwidth}|p{0.3\textwidth}| }
\hline
&&\\*[-0.9em]

\textbf{main} & \textbf{child} & \textbf{sem{\_}value} \\*
&&\\*[-0.9em]
\hline
\endhead
&&\\*
wait on semaphore&& \multicolumn{1}{r|}{1}\\*
semaphore value >= 0, so proceed&& \multicolumn{1}{r|}{0}\\*
program exits&& \multicolumn{1}{r|}{0}\\*
&&\\*
\hline
\end{longtable}

In order to delay the main thread's exit until the child thread has run, the semaphore should be given an initial value of \texttt{0}. Decrementing this will leave a negative value, and the main thread will be unable to progress. The signal to continue is sent by the child thread, restoring the semaphore to the zero value and unblocking the main thread.\\

\begin{longtable}{ |p{0.3\textwidth}|p{0.3\textwidth}|p{0.3\textwidth}| }
\hline
&&\\*[-0.9em]

\textbf{main} & \textbf{child} & \textbf{sem{\_}value} \\*
&&\\*[-0.9em]
\hline
\endhead
&&\\*
wait on semaphore&& \multicolumn{1}{r|}{0}\\*
semaphore value < 0, so wait&& \multicolumn{1}{r|}{-1}\\*
sleep...&& \multicolumn{1}{r|}{-1}\\*
&child thread runs& \multicolumn{1}{r|}{-1}\\*
&child posts the semaphore& \multicolumn{1}{r|}{-1}\\*
resumed as semaphore value >= 0&& \multicolumn{1}{r|}{0}\\*
program exits&& \multicolumn{1}{r|}{0}\\*
&&\\*
\hline
\end{longtable}

It is possible that the child thread runs and increments the semaphore before the main thread begins waiting. It may seem that the \emph{signal} sent by the child is not received, however a record of it is preserved in the semaphore, ready for the main thread to read.\\

\begin{longtable}{ |p{0.3\textwidth}|p{0.3\textwidth}|p{0.3\textwidth}| }
\hline
&&\\*[-0.9em]

\textbf{main} & \textbf{child} & \textbf{sem{\_}value} \\*
&&\\*[-0.9em]
\hline
\endhead
&&\\*
&child thread runs&\multicolumn{1}{r|}{0}\\*
&child posts the semaphore&\multicolumn{1}{r|}{0}\\*
main thread runs&&\multicolumn{1}{r|}{0}\\*
semaphore value >= 0, so proceed&& \multicolumn{1}{r|}{0}\\*
program exits&& \multicolumn{1}{r|}{0}\\*
&&\\*
\hline
\end{longtable}

With the correct semaphore setup, the desired behaviour is reached.\\

\begin{minted}[ frame=single, framesep=8mm, rulecolor=RuleGray, baselinestretch=1.3, fontsize=\small, breaklines, fontfamily=tt]{text}
parent entering
child entering
child exiting
parent exiting
\end{minted}

This was a basic example, demonstrating little other than thread ordering with the bare minimum of threads. In the technical solution, I have used multiple semaphores in a \emph{hand-over-hand} fashion to control access to a shared datastructure.\\
