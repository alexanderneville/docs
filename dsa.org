#+TITLE: Data Structures and Algorithms

* Introduction

An /algorithm/ is fundamentally a sequence of instructions for solving a problem. There is no single specification or language for writing and interpreting algorithms. If a computer is to execute the steps of an algorithm, the algorithm in question must be /implemented/ in a computer /program/. Programs are written in formal languages, which are inherently more detailed and precise than an algorithm and conform an exact syntax as understood by a compiler. Algorithms provide /abstraction/ from the implementation details of programming, making it possible to describe and reason about problems more easily.

The more abstractly an algorithm is defined, the more ambiguous it becomes. It can be difficult to understand several critical and non-critical aspects of an algorithm expressed in a few English sentences. For example, the relationship between input and output and its structure, known as the algorithm's /specification/ may be largely undefined for such an algorithm. For computer science applications, /pseudocode/ is often used as an abstract method for describing algorithms in a way that preserves some focus on the implementation and conveys more detail.

Central to any computer program is data, the information to be processed. For any program, the organisation of this data is a critical part of the solution. The role of a /data structure/ is to manage how a program's information is stored and accessed.

In computer programming, a /data type/ is a set of possible values and supported operations for a given piece of data. Many languages include a type system, which ensures a type is associated with each term or variable within a program. A language's syntax may feature explicit type annotations, where each variable is labelled with a type, while variable types may be implicitly typed in others. A /primitive/ data type is provided by the language implementation. The primitive types of one language may be similar or dissimilar to those of another. Primitive types may also be referred to as /atomic/, /basic/, /fundamental/ or /built-in/. All other types, including data structures (which are types) are said to be /user-defined/.

An /Abstract Data Type/ is defined more loosely by the set of supported operations on it, or in other words its interface, rather than its implementation. An ADT is not concerned with the representation of data and the only information learnt from an ADT specification is the set of operations made available (public). This is called information hiding or /encapsulation/, an important principle in programming. /Concrete/ data structures implement abstract data types.

Confusingly, there isn't a single standard or specification for any given abstract data type and the functions it should expose; there exists many different ways to define an ADT. For any abstract data type that can be conceived, its specification should contain the minimal set of operations or /functions/ on it, from which all other operations on the type can be /derived/. Any additional functions are redundant if they can be implemented with the existing parts of the ADT. Highly abstract mathematical type definitions borrow heavily from the techniques of the functional programming paradigm, revealing very little indeed about ADT implementation. Such /inductive/ approaches to abstract type definition are more correct in the mathematical sense, at the cost of being less useful types in programming. Languages such as Java provide very different, more substantial ADT definitions.

* Lists

A /list/ is an abstract data type representing a finite number of ordered values in which the same value may appear more than once. Lists are frequently implemented with array and linked list data structures, among others. 

** Arrays

An array is a simple data structure which stores items in sequential memory locations. Arrays can be written as a collection of items inside square brackets as follows, $[4,3,7,2,9,1,8,6]$. This array has 8 elements, so its /size/ would be considered 8. If this array was assigned to a variable $a$, its elements can be accessed through their /index/ $i$ and the indexing notation $a[i]$. Indexing usually begins from 0, so the valid indices for this list are $0 \ldots 7$, as seen in [[fig:array]].

#+CAPTION: An array and its indices
#+ATTR_LATEX: :placement [H] :scale 1
#+NAME: fig:array
[[./res/array.svg]]

Each element may be accessed /sequentially/ by incrementing or decrementing the index as required, or at random by taking any index - assuming it is a valid index. For the indexing operation to be effective, each item in the array is required to be the same size, which means in practice array elements are of the same type. In a language such as C, the array $a$ would be considered a pointer to the memory location of the beginning of the array and the index $i$ is an offset from the start of the array. This means that $a[0]$ points to the beginning of the array, the /zeroth/ element. For non zero indices, this offset is calculated by multiplying the index by a certain number of bytes equal to the size of each item, $c$. As such the address of $a[i]$ is $a + c \times i$.

** Arrays, Iteration & Invariants

Index access to array elements, makes the array data structure conducive to sequential access. Iteration over the index variable, which is repeatedly incremented, enables array elements to be accessed and processed at run time. A /loop/ is the programming construct facilitating iteration. It is commonly expressed in pseudocode and C like syntax.

#+ATTR_LATEX: :float t :placement [H]
#+CAPTION: Iteration in pseudocode and C like syntax
#+begin_src text
For i <- 0 ... (N - 1) Do
    use value i

for (i = 0; i < N; i++) {
    // use value i 
}
#+end_src

In both of these examples $N$ is an invariant. It does not change in the loop body or anywhere else in the program. Invariants prevent common errors such as accessing indices beyond the end of an array. Due to the static nature of arrays (arrays are allocated once at a certain size), array size is a common program invariant.

** List ADT

As an abstract data type, a list is defined in terms of its public functions. A minimal list type can be defined with two /constructors/, where $E$ is an element of a list and $L$ is a list.

- $\text{emptylist} () \rightarrow L$
- $\text{prepend} (E, L) \rightarrow L$

Any list can be created from a single empty list and a series of prepend operations. The list $[1,2,3]$ is created with the expression =prepend(1, prepend(2, prepend 3, emptylist()))=. This is called an /inductive/ type definition, relying on the repeated application of the inductive step =prepend(E,L)= on the base case =emptylist()=. Any useful data type has accessor methods. In the absence of array indices, any list element can be retrieved with a combination of two /selectors/.
  
- $\text{head} (L) \rightarrow E$
- $\text{tail} (L) \rightarrow L$

The functions head and tail are not defined for the empty list. An additional function is required to determine whether a given list is empty.

- $\text{isemptylist} (L) \rightarrow \text{T}|\text{F}$

With this set of functions, the following expressions are true.

- $\text{isemptylist} (\text{emptylist} ())$
- $\text{not isemptylist} (\text{prepend} (e, l))$
- $\text{head} (\text{prepend} (e, l)) == e$
- $\text{tail} (\text{prepend} (e, l)) == l$

** Recursion & Derived List Procedures

Iteration is the logical and convenient method to process a collection of elements in an array, due to indexing. With the abstract list definition and in many list implementations, such as linked lists, there is no index. It becomes more convenient to process lists with recursion, although any recursion can be expressed as an iteration. Because of the inductive construction of the list type, obtaining the last element requires every element in the list is processed or /traversed/. The function =last= returns the last element of the list =l=.

#+ATTR_LATEX: :float t :placement [H]
#+CAPTION: Return the last element of a list
#+begin_src text
last(L:l) -> E {
    if (isemptylist(tail(l)) return head(l);
    return last(tail(l));
}
#+end_src

This implementation ensures that the =last= function is not recursively applied to an empty list, although if the function is initially called on an empty list, =tail= will be passed an empty list, for which it is undefined. This situation could be handled with an additional condition in the =last= function.

#+ATTR_LATEX: :float t :placement [H]
#+CAPTION: The same, with error handling
#+begin_src text
last(L:l) ->E {
    if (isemptylist(l)) {
        error();
    } else if (isemptylist(tail(l)) {
        return head(l);
    } else {
        return last(tail(l));
    }
}
#+end_src

Appending, rather than prepending, to a list is also a derived function on this list. For the sake of simplicity, appending a single element =x= to the end of a list =l1= can be achieved by passing =prepend(x,emptylist())= in place of =l2=.

#+ATTR_LATEX: :float t :placement [H]
#+CAPTION: Append one list to another
#+begin_src text
append(L:l1,L:l2) -> L {
   if (isemptylist(l1)) return l2;
   return prepend(head(l1), append(tail(l1), l2))
}
#+end_src

Modifying the base case slightly, it is possible to write a function which appends a single element to the end of a list.

#+ATTR_LATEX: :float t :placement [H]
#+CAPTION: Append an element to a list
#+begin_src text
append(L:l,E:x) -> L {
   if (isemptylist(l)) return prepend(x, emptylist());
   return prepend(head(l), append(tail(l), x))
}
#+end_src

Many of these derived functions are slow and inefficient. In practice it might be easier to use some of the underlying implementation details of a data structure to accelerate more complicated operations and expose more functions on a type than are technically required. Also absent in this ADT are /mutator/ functions, which destructively modify a list. In the functional inductive approach to ADT specification lists are immutable and each function returns a new list. It is convenient to return a new immutable list for every operation on a given list, as it makes a program safe and predictable. There is no concept of state that can be modified during the course of the program, so applying the same function on the same set of arguments will produce the same output. The trade off is the space and time complexity associated with repeated memory allocation and traversal, yet another reason why complex data types in programming languages and libraries are truly abstract.

** Linked Lists

Lists contain a finite number of elements, but theoretically this number has no upper bound. Lists on computers are practically limited in size by the amount of space or memory available. A list type must allocate space for the data elements it contains. If the maximum size of a list is known in advance, an array may be the most effective way too implement a list. Otherwise, if the size of a list varies at run time, a more dynamic implementation is required. The first such implementation is the /linked list/.

The most simple linked list is composed of a sequence of /nodes/ or /two-cells/. Each contains an element (or a reference to an element) and a /reference/ to the next node. figure [[fig:abstract_llist]] is the most abstract graphical representation of the list $[8,4,1,7,3,6]$. The second of each two-cell is a reference to the next node, not the first cell of the next node, as is often depicted.

#+ATTR_LATEX: :placement [H] :scale 1
#+CAPTION: An abstract graphical linked list illustration
#+NAME: fig:abstract_llist
[[./res/abstract_llist.svg]]

In any implementation, the nodes of a linked list are themselves a type, in most cases hidden from the external interface of a linked list. The fields or cells of a node may contain a reference to the element, or hold a copy of the element in place. The second field however *must* contain a reference to the next node. In a language such as Java, a node may be a user-defined object.

#+begin_src java
class Node {
    int data;
    Node next;
}
#+end_src

It would appear that a node contains the next node directly, although this isn't the case. In this Java source =Node= is a reference type. The allocation of a new node and pointer logic are all handled implicitly. This is the equivalent of a C structure containing a pointer to another structure of the same type.

#+begin_src c
struct Node {
    int data;
    struct Node * next;
}
#+end_src

It is not possible for a C structure to contain a field of its own type as it would appear in Java, as the structure definition is incomplete at the point the field is declared. Attempting to declare such a structure is impossible, it would require an infinite amount of memory.

#+begin_src c
struct Node {
    int data;
    struct Node next; // invalid
}
#+end_src

The same linked list is more rigorously represented as a /structure/ containing a start pointer to a node and subsequent nodes are referenced by a field of the previous node, as in figure [[fig:llist]], at the cost of revealing more implementation details. This removes the confusion of what the second cell of each two-cell references in the figure [[fig:abstract_llist]]. The value of each node can also be written more compactly inside the first two cell.

In short there are practical reasons to model a whole linked list as a data type in its own right and there are different stylistic approaches to illustrating linked lists. Compare figures [[fig:abstract_llist]] and [[fig:llist]].

#+ATTR_LATEX: :placement [H] :scale 1
#+CAPTION:A more concrete linked list illustration
#+NAME: fig:llist
[[./res/llist.svg]]

In keeping with the inductive type definition, it is unnecessary to illustrate both nodes and a dedicated list object. It is still possible to write a linked list implementation where the only user defined type is a node. It is useful to have a separate linked list type to store additional metadata about the list such as size or tail pointers in the case of a /queue/. In either case, such a type can be abstracted away and it is assumed there is some reference to the first node somewhere in the program when a linked list is expressed as a simple collection of nodes.

As an example of a linked list variation, the last node may reference the first node, the same as the start pointer. Some program may need to begin performing an operation at some arbitrary point within the list, other than the first element, in which case this change is helpful. This is called a /circular/ linked list.

#+ATTR_LATEX: :placement [H] :scale 1
#+CAPTION: A circular linked list
#+NAME: fig:circular_llist
[[./res/circular_llist.svg]]

A /queue/ is a linked list with an additional two-cell pointing to the front and rear nodes.

#+ATTR_LATEX: :pla cement [H] :scale 1
#+CAPTION: A linked list with start and rear pointer
#+NAME: fig:queue_llist
[[./res/queue_llist.svg]]

A node could contain a pointer for the next and previous nodes, called a /doubly-linked/ or /double linked/ list. If the list additionally implements the circular property, it can behave as a queue (the rear pointer is the previous node of the start).

#+ATTR_LATEX: :placement [H] :scale 1
#+CAPTION: A circular doubly-linked list
#+NAME: fig:double_circular_llist
[[./res/double_circular_llist.svg]]

** Dynamic Arrays

An array is allocated once with a given size. An array is an ideal container type, if the data to be stored is of the same form and the quantity of data is know at compile time. If the maximum size of a data structure required to store some elements is not known at compile time, it is still possible to use an array with some additional caveats. An array capable of growing in this way is known as a /dynamic array/ or /array list/.

Array size is a type of program invariant, although an array list is characterised by two variables: the current maximum /capacity/ of the array and the /size/ or number of elements. Insertion when size is less than capacity is $O(1)$. Insertion when size is equal to capacity is of order $O(n)$, the array must be reallocated to make space for more elements. Figure [[fig:array_list]] depicts the growth of a dynamic array, reallocation is indicated with an arrow.

#+ATTR_LATEX: :placement [H] :scale 1
#+CAPTION: Reallocation of an array list
#+NAME: fig:array_list
[[./res/array_list.svg]]

Using an array to implement a data structure has the potential to make insert/remove/access operations faster, although there are drawbacks with this approach. For any data structure that is allocated at a given size, such as an array, it is possible the data structure becomes full, this condition needs to be handled. Array lists can have worse space complexity if more space than is required is allocated and under certain conditions the time complexity of an operation may be worse, such as the reallocation of memory. 

** Stacks

A stack is an abstract data type organises data in /First-In-Last-Out (FILO)/ or /Last-In-First-Out (LIFO)/ manner. The most recently inserted item is the first to be removed from a stack. A stack can be defined inductively with the constructors =emptystack= and =push=, the conditional =isemptystack= and the selectors =top= and =pop=. 

- $\text{emptystack} () \rightarrow S$
- $\text{push} (E,S) \rightarrow S$
- $\text{isemptystack} (S) \rightarrow T|F$
- $\text{top} (S) \rightarrow E$
- $\text{pop} (S) \rightarrow S$

In this stack definition, which does not mutate the state of one stack, instead creating and returning new stacks as required, =top= returns the first element of a stack and =pop= returns the remainder of stack, without the first element. For most practical purposes, a single stack is used and changed destructively, in which case =push= and =pop= have different definitions.

- $\text{push} (E,S)$
- $\text{pop} (S) \rightarrow E$

This version of =pop= removes and returns the first element of a stack. The state of the original stack is changed to reflect the result of the operation. There is no need to create and return a new stack.

A stack is very easily implemented with a singly linked list. Items are inserted and removed from the front. In figure [[fig:stack_push_pop]] the integers 7, 2 and 9 are pushed onto the stack. Items are retrieved in the reverse order of their insertion. The integers pushed onto the stack are popped from the front and returned in the order 9, 2, 7. In this example, the whole process mutates a single stack, which is explicitly depicted in the figure.

#+ATTR_LATEX: :placement [H] :scale 1
#+CAPTION: Stack operations, push and pop
#+NAME: fig:stack_push_pop
[[./res/stack_push_pop.svg]]

It is also possible to implement a stack as an array. Items are added and removed from the rear. The rear position is calculated from the size of the stack (the number of element in the stack). Accessing any element in an array can be done in constant time. The stack implementation also maintains the maximum stack size (size of the underlying array). If the size of the stack is the allocated size of the array, the stack is considered /full/. Pushing and further elements onto the stack will result in a state known as /stack overflow/. A dynamic array stack implementation can be used to avoid this condition.

#+ATTR_LATEX: :placement [H] :scale 1
#+CAPTION: Stack implemented as an array
#+NAME: fig:stack_array
[[./res/stack_array.svg]]

** Queue

Unlike a stack, queue items are removed in the order they were originally inserted, called a /First-In-First-Out (FIFO)/ or /Last-In-Last-Out (LILO)/ data structure. Queues share a very similar inductive definition to stacks, though their implementations differ.

- $\text{emptyqueue} () \rightarrow Q$
- $\text{push} (E,Q) \rightarrow Q$
- $\text{isemptyqueue} (S) \rightarrow T|F$
- $\text{top} (Q) \rightarrow E$
- $\text{pop} (Q) \rightarrow Q$

The role of =top= and =pop= are achieved through the mutator =dequeue=, while =enqueue= performs an operation analogous to =push=, manipulating an existing queue. 

- $\text{enqueue} (E,Q)$
- $\text{dequeue} (Q) \rightarrow E$

For an efficient queue implementation, start and rear pointers must be maintained. With these two references, items can be enqueued at either the start or rear of the linked list in constant time. Items can only be dequeued from the start of a linked list in constant time. To dequeue from the rear, the rear pointer must be updated to point to the penultimate (new rear) element. With a singly linked list, this requires iteration from the start, $O(n)$ complexity. Therefore, the most effective way to use a linked list to implement a queue is enqueue at the rear and dequeue from the front, illustrated in figure [[fig:queue_enqueue_dequeue]].

#+ATTR_LATEX: :placement [H] :scale 1
#+CAPTION: Queue operations, enqueue and dequeue
#+NAME: fig:queue_enqueue_dequeue
[[./res/queue_enqueue_dequeue.svg]]

A queue can be implemented as an array, with three additional variables: =front=, =size= and =capacity=. So that the bounds of the array are not exceeded, =front + size - 1 < capacity= must hold. As items are dequeued, the front pointer is incremented and the number of available slots decreases. It is possible that =front + size - 1= is equal to the maximum capacity of the array, but most of the array is empty. The simple solution to this problem is moving the occupied cells to the beginning of the array, either when it is necessary or after each dequeue operation. A slightly different implementation is preferable.

As successive enqueue and dequeue operations are conducted the occupied portion of the queue shifts along the allocated space of the array. When the rear element is at index =capacity -1=, adding an element to the queue places it at index =0=, the queue wraps on the boundary. Now the array only becomes full when the size of the queue is equal to the capacity of the array. In a circular array, a queue occupies the indices:

- =front, ..., front + size - 1= if =front + size - 1 < capacity=
- =front, ..., capacity - 1= and =0, .., front + size - capacity - 1= if =front + size > capacity=

In figure [[fig:circular_queue]], a queue of size three occupies different portions of the array. The front pointer is indicated with an arrow.
  
#+ATTR_LATEX: :placement [H] :scale 1
#+CAPTION: Queue implemented as a circular array
#+NAME: fig:circular_queue
[[./res/circular_array.svg]]

An example implementation of a queue with a circular array contains four functions, two conditionals =isemptyqueue= and =isfullqueue=, a constructor =enqueue= (emptyqueue is omitted here) and a selector =dequeue=.

#+ATTR_LATEX: :float t :placement [H]
#+CAPTION: Circular queue implementation
#+begin_src text
record E { ... };
record Q {
    int size;
    int capacity;
    E[] arr;
};
isemptyqueue(Q:q) -> T|F {
    return q.size == 0;
}
isfullqueue(Q:q) -> T|F {
    return q.size == q.capacity;
}
enqueue(E:e, Q:q) {
    if (isfullqueue(q)) THROW ERROR;
    q.arr[(q.front + q.size++) mod q.capacity] = e;
}
dequeue(Q:q) -> E:e {
    if (isemptyqueue(Q)) THROW ERROR;
    E e = q.arr[q.front];
    q.front = q.front + 1 mod q.capacity;
    q.size--;
    return e;
}
#+end_src

* Algorithms
** Performance & Complexity

The /performance/ of an algorithm refers to its resource usage: memory consumption, running time or both. Both factors are important when choosing an algorithm, but more often than not it is time complexity being measured.

The running time of an algorithm is not an effective method of quantifying its performance, as the same algorithm run on different machines, or implemented in a different language, may not run for the same length of time. Instead, the number of steps taken to solve a problem is a more consistent measure of time performance. Space performance is easier to calculate as the number of bytes required in memory for the algorithm to run. 

Performance itself is not a useful metric, it does not capture how the number of steps increases with the size of the problem. Performance parameterised by input size is know as /complexity/. An algorithm might take $n$ steps on a input size of $n$, or $n^2$ steps on the same input size $n$.

Some algorithms may take longer under different input conditions, for example linear search is much faster if the element to find is first in a list than if the element doesn't appear in the list at all. The complexity of this algorithm can be measured under different cases, eg. in the /best case/, where the element to find is at the start of the list (constant time complexity); the /average case/, where the element is in the middle of the list ($n/2$ complexity) or the /worst case/, where the element to find doesn't appear in the list ($n$ complexity).

*** Big $O$ Notation

Presented with a function expressing the exact complexity of an algorithm, /big O/ notation simplifies the complexity to its most significant headline complexity. If $f(n)$ is the sum of many terms, then only the term with the highest growth rate is taken. Any constant factor, coefficient or term that doesn't depend on the input size (any overhead) can be ignored. The resulting expression is known as a /complexity class/. 

As an example, the function $f(n) = 3n^2 + 6n +10$ can be simplified to the complexity class $n^2$. $f(n)$ is /big O/ of $n^2$, written more simply $O(n^2)$. The function $f(n)$ is said to belong to a complexity class, often written $f(n) = O(n^2)$. Note that in $O(g(n))$, $O$ is not a function, it is shorthand for the /"class of functions with complexity of order $g(n)$"/, the same expression could also be written $f(n) \in O(g(n))$. 

In formal terms, a function $f$ belongs to complexity class $O(g)$ if there exists a constant $C > 0$ such that for all $n \ge n_0$, $f(n) \le Cg(n)$; At some point the function $g$ is larger than the function $f$.

\[f(n) = O(g(n)) \iff |f(n)| \le |Cg(n)|\]

A function is /at most as fast growing/ as (grows no faster than) the complexity class it belongs to. A function also belongs to all the complexity classes larger than it, although this is less informative. A function with complexity $O(n)$ also belongs to $O(n^2)$ and $O(n^3)$.

\[O(1) \subseteq O(\log_2 \log_2 n) \subseteq O(\log_2 n) \subseteq O(n) \subseteq O(n log_2 n) \subseteq O(n^2) \subseteq O(n^3) \subseteq O(2^n)\]

*** Little $o$ Notation

Little $o$ notation is a stricter upper bound for a function's complexity. Functions that are $o(g(n))$ are also $O(g(n))$, but the opposite is not always true. Little o complexity means that $g(x)$ /grows faster than/ $f(x)$.

\[f(n) = o(g(n)) \iff \lim_{n \to \infty} \dfrac{f(n)}{g(n)} = 0\]

The function $2n^2$ is $O(n^2)$ in complexity and also belongs $O(n^3)$, but $2n^2 \neq o(n^2)$. Therefore $2n^2 = o(n^3)$. $o(g(n))$ is said to /dominate/ $f(n)$.

\[\lim_{n \to \infty} \dfrac{2n^2}{n^2} = 2\]

\[\lim_{n \to \infty} \dfrac{2n^2}{n^3} = \lim_{n \to \infty} \dfrac{2}{n} = 0\]

*** Omega Notation

A lower bound on the growth of $f(n)$. A function grows /at least as fast/ as $g(n)$.

\[f(n) = \Omega(g(n)) \iff |f(n)| \ge |Cg(n)|\]

*** Theta Notation

As an upper bound $f(n) = \Theta(g(n))$ is similar to big $O$, but additionally specifies a lower bound by a second constant multiple of $g(x)$.

\[f(n) = \Theta(g(n)) \iff C_1g(n) \le f(n) \le C_2g(n)\]
\[f(n) = \Theta(g(n)) \iff f(n) = O(g(n)) \land f(n) = \Omega (g(n))\]

The functions $f$ and $g$ grow just /as fast as/ each other, $f(n) = O(g(n))$ and $g(n) = O(f(n))$. In big $O$ notation, it is possible for a function to be part of multiple larger complexity classes. This is not possible in theta notation.

\[3n^2 + 2n + 1 = O(n^2)\]
\[3n^2 + 2n + 1 = \Theta(n^2)\]
\[3n^2 + 2n + 1 = O(n^3)\]
\[3n^2 + 2n + 1 \neq \Theta(n^3)\]

*** Asymptotically Equal

Asymptotically equal complexity has the same relationship with Theta, as little $o$ has to big $O$. It is a stricter upper and lower bound.

\[f(n) \sim g(n) \iff \lim_{n \to \infty} \dfrac{f(n)}{g(n)} = 1\]

*** Amortized Complexity

The measures of best, average and worst case complexity quantify the performance of one operation on a given input size. In some cases, the difference between the average and worst case scenario is small and large in other situations. If the worst case scenario occurs infrequently, big $O$ may not be an accurate assessment of the algorithm's complexity.

Amortized complexity is measured over a number of successive operations. This measure is ideal for describing algorithms which perform one or more expensive operations to accelerate subsequent operations.

** Searching & Sorting
*** Comparison-based sorting

A sorting function orders elements according to a /comparison function/. Sometimes, as is the case with numeric records, a bespoke comparison function is not explicitly required.

*** Sorting Strategies

1. *Selection:* Find the correct value for a given position in the output data structure.
2. *Insertion:* Find the correct position in the output data structure for a given element from the input space.
3. *Exchange:* If two elements in the input space are out of order, swap their positions.
3. *Divide & Conquer:* Recursively divide the input into smaller sub-problems and reassemble, preserving order into the output data.

*** Stability

The relative order of two records with the same key is preserved by a /stable/ sorting algorithm. A combination of stable sorting algorithms can form a sorting /pipeline/ and underlying elements are ordered by a combination of conditions.

*** Bubble Sort

Bubble sort is a stable, in-place, comparison-based, exchange sort algorithm. After $i$ iterations, $i$ elements are in the correct position. In the worst and average cases, bubble sort performs $O(n^2)$ comparisons and swaps $O(n^2)$ elements. In the best case, bubble sort performs $O(n)$ comparisons and $O(1)$ swaps, efficient for fully or mostly sorted lists. The space complexity is constant (no auxiliary space required).

#+ATTR_LATEX: :float t :placement [H]
#+CAPTION: Two common bubblesort implementations
#+begin_src text
bubblesort(a[],n) {
    for (i = 0; i < n - 1; i++)
        for (j = 0; j < n - i - 1; j++)
            if (a[j] > a[j+1])
                swap(a[j], a[j+1]);
}
bubblesort(a[],n) {
    for (i = 1; i < n; i++)
        for (j = n - 1; j >= i; j--)
            if (a[j] < a[j-1])
                swap(a[j], a[j-1]);
}
#+end_src
          
*** Insertion Sort

Insertion sort may be implemented as a stable, in-place, comparison-based, insertion sort algorithm. After $i$ iterations, $i$ elements are correctly ordered relative to each other (but not necessarily in their final position). In the worst and average cases, insertion sort performs $O(n^2)$ comparisons and swaps $O(n^2)$ elements. In the best case, bubble sort performs $O(n)$ comparisons and $O(1)$ swaps, ideal for fully or mostly sorted lists. The space complexity (no auxiliary space required) is constant if the output array grows inside the input array.

#+ATTR_LATEX: :float t :placement [H]
#+CAPTION: Insertion sort implementation
#+begin_src text
insertionsort(a[],n) {
    for (i = 1; i < n; i++){
        j = i;
        while (j > 0 && a[j] < a[j-1]) {
            swap(a[j], a[j-1]);
            j--;
        }
    }
}
#+end_src

*** Selection Sort

Selection sort may be implemented as a stable, in-place, comparison-based, selection sort algorithm. After $i$ iterations, $i$ elements are in the correct position. In the worst and average cases, insertion sort performs $O(n^2)$ comparisons and swaps $O(n)$ elements. In the best case, bubble sort performs $O(n^2)$ comparisons and $O(1)$ swaps. The space complexity is constant (no auxiliary space required) if the output array grows inside the input array.

#+ATTR_LATEX: :float t :placement [H]
#+CAPTION: Selection sort implementation
#+begin_src text
selectionsort(a[],n) {
    for (i = 0; i < n - 1; i++) {
        k = i;
        for (j = i + 1; j < n; j++)
            if (a[j] < a[k]) k = j;
        if (k != i) swap(a[i], a[k]);
    }
}
#+end_src

*** Merge Sort

Merge sort is a more efficient stable, comparison-based divide-and-conquer sorting algorithm. The list to be sorted is recursively divided into two sub-lists until the base case - the list of length one - is reached. Sub-lists are merged together by taking the smallest of either sorted sub-list as the next element of the sorted list. Merge sort is $\Theta(n \log n)$ in the best, worst and average cases. Merge sort additionally requires $O(n)$ of auxiliary space to act as a buffer for merging sub-lists if the sort acts on an array.

#+ATTR_LATEX: :float t :placement [H]
#+CAPTION: Merge sort implementation
#+begin_src text
mergesort(a[], l, r) {
    if (l < r) {
        m = (l + r) // 2;
        mergesort(a, l, m);
        mergesort(a, m + 1, r);
        mergesublists(a, l, m, r);
    }
}
mergesublists(a[], l, m, r) {
    n = (r - l) + 1
    new b[n];
    i = l;
    j = m + 1;
    k = 0;
    while ((i <= m) && (j <= r)) {
        if (a[i] < a[j]) b[k++] = a[i++];
        else b[k++] = a[j++]
    }
    while (i <= m) b[k++] = a[i++];
    while (j <= r) b[k++] = a[j++];
    for (x = 0; x < n; x++) a[l + x] = b[x];
}
#+end_src

*** QuickSort

Quicksort is an efficient comparison-based divide-and-conquer sorting algorithm. At each recursive level a partition element is chosen according to some algorithm and all smaller elements are arranged to the left and all larger elements are arranged to the right. A recursive call is made to sort each sub-list, to the left and right of the partition.

The choice of pivot is a significant factor in the performance of quicksort. If an extreme (large or small) element is chosen, one partition will have considerably more elements than than the other. Ideally the median value is chosen as the pivot. In the best and average cases, where the pivot is close to the median, complexity of quicksort is $O(n \log n)$. In the worst case the complexity is $O(n^2)$.

#+ATTR_LATEX: :float t :placement [H]
#+CAPTION: Unstable quicksort implementation
#+begin_src text
quicksort(a[], l, r) {
    if (l < r) {
        p = partition(a[], l, r);
        quicksort(a[], l, p - 1);
        quicksort(a[], p + 1, r);
}
}
partition(a[], l, r) {
    p = choose_pivot(...);
    swap(a[p], a[r]);
    i = l - 1;
    for (j = l; j < r; j ++) {
        if (a[j] < a[r]) {
            swap(a[j], a[++i]);
        }
    }
    swap(a[r], a[i+1]);
    return i + 1;
}
partition(a[], l, r) {
    p = choose_pivot(...);
    swap(a[p], a[r]);
    i = l;
    j = r - 1;
    while (i <= j) {
        while(i <= j && a[i] <= a[r]) i ++;
        while(j >= i && a[j] >= a[r]) j --;
        if (i < j) swap(a[i], a[j]);
    }
    swap(a[r], a[i]);
    return i;
}
#+end_src

The most simple quicksort algorithm is not stable. Quicksort can be made stable by introducing a buffer to store elements greater than and equal and occurring to the right of the pivot. Once these elements have been collected, they are added to the appropriate index of the original sub-list. Using a buffer introduces $O(n)$ auxiliary space complexity.

#+ATTR_LATEX: :float t :placement [H]
#+CAPTION: Stable quicksort implementation using a buffer
#+begin_src text
partition(a[], l, r) { // stable sort
    p = choose_pivot(...);
    pv = a[p];
    n = (r - l) + 1
    new b[n];
    i = l;
    j = 1; // reserve 0 index for pivot
    for ( k = l; k <= r; k++) {
         if (k = p) b[0] = a[k];
         else if (a[k] < pv || (a[k] == pv && k < p)) a[i++] = a[k];
         else b[j++] = a[k];
    }
    for (m = 0; m < j; m++)
        a[i++] = b[m];
    return r - j + 1;
}
#+end_src

*** Heap Sort

* Trees

A tree data structure models the abstract tree type. A conceptual tree is a hierarchical structure with an explicit /root/. In the context of data structures and algorithms, a tree is a type of connected graph, composed of /nodes/ and /edges/ with no cycles and exactly one route between the root and any other node. Trees are generally depicted with the root node at the top and all other nodes arranged into discrete levels as in figure [[fig:general_tree]].

#+ATTR_LATEX: :placement [H] :scale 1
#+CAPTION: A general tree
#+NAME: fig:general_tree
[[./res/general_tree.svg]]

A tree is composed of /nodes/ which contain data and references to other connected nodes. A node in a tree may have any number of connected /child/ nodes. Every tree node is referenced by exactly one /parent/ node, except in the case of the root node which does not have a parent node. Each node can be treated as the root of its own sub-tree. The sub-tree with any tree node as its root has all the same properties as the tree the sub-tree root belongs to.

- A /descendant/ of a node is any node that can be reached traversing from parent to child, repeatedly if necessary. A node can have as many or more descendants than children.
- An /ancestor/ of a node is any node that can be reached by traversing from child to parent any number of times (at least once). A non-root node always has exactly one parent, but may have many nodes as ancestors.
- A /path/ is a sequence of connected edges between two nodes via any number of other nodes.
- The /depth/ or /level/ of a node in a tree is the length of the path from the root to the node. The root node has a depth of 0.
- A node is considered /internal/ if it has one or more connected children.
- A node is said to be a /leaf/ node if it has exactly zero connected children.
- A node's /siblings/ are any nodes with the same parent. It is possible for a node to have no siblings.

The /height/ of a tree is the length of the longest path from the root to any node, /ie./ the maximum depth of any node. The /size/ of a tree is the number of nodes it contains. An empty tree has 0 nodes and so a size of 0 and in conventional notation a height of -1.

** Inductive Type Definition

Each node in a tree is the root of a sub-tree rooted at that point. A tree can be built /inductively/ from the special /empty tree/ which has no value and no children. Larger trees are built with a value and a list of children, a list of trees is called a /forest/ $F$. For a general tree, the ADT can be defined:

- $\text{emptytree}() \rightarrow T$
- $\text{maketree}(E,F) \rightarrow T$
- $\text{isemptytree}(T) \rightarrow \top|\bot$
- $\text{children}(T \rightarrow F)$
- $\text{valuetree}(T \rightarrow E)$

A common implementation of the general tree with no constraint on the number of children of each node is the /sibling list/, illustrated in figure [[fig:sibling_list]]. Each node has in addition to its value, a pointer to the list of its children and a second pointer to its next sibling.

#+ATTR_LATEX: :placement [H] :scale 1
#+CAPTION: A general tree as a sibling list
#+NAME: fig:sibling_list
[[./res/sibling_list.svg]]

** Binary Trees

A /binary tree/ is a type of /N/-ary tree in which each node has at most two children as in figure [[fig:binary_tree]]. The general tree definition holds for binary trees, but the binary nature of the tree is not enforced. A proper binary tree definition states that building a new tree requires two child trees rather than a list of trees of unspecified length. Similarly, a binary tree node's children are returned by two specific accessor:

- $\text{emptybinarytree}() \rightarrow T$
- $\text{makebinarytree}(E,T,T) \rightarrow T$
- $\text{isemptybinarytree}(T) \rightarrow \top|\bot$
- $\text{leftbinarytree}(T) \rightarrow T$
- $\text{rightbinarytree}(T) \rightarrow T$
- $\text{valuebinarytree}(T) \rightarrow E$

#+ATTR_LATEX: :placement [H] :scale 1
#+CAPTION: A binary tree
#+NAME: fig:binary_tree
[[./res/binary_tree.svg]]

A binary tree or any /N/-ary tree can be implemented as a sibling list, but this is usually unnecessary if the maximum number of children is know. Without changing how a node is defined, the first pointer can be used to point to the left child and the second pointer can be used to point to the right child rather than the list of children and the next sibling, as was the case before.

#+ATTR_LATEX: :placement [H] :scale 1
#+CAPTION: A binary tree as an array
#+NAME: fig:binary_tree_array
[[./res/binary_tree_linked.svg]]

For fast access and traversal without repeated dereferencing, values can be added to a position in an array. The children of each node are accessed by index $2^i$ and $2^i+1$ where $i$ is the index of the current node. To make the arithmetic work, the zeroth entry in the array is left empty. This approach limits the maximum size of the tree, unless the array is dynamic, and is wasteful if the leaves are found at different levels.

#+ATTR_LATEX: :placement [H] :scale 1
#+CAPTION: A binary tree as an array
#+NAME: fig:binary_tree_array
[[./res/binary_tree_array.svg]]

** Quad trees
** Binary Search Trees

A binary search tree is a type of binary tree with the additional constraint that a node's children are in order; keys with a lower value are inserted into the left and keys with higher values are inserted into the right subtree. Any subtrees rooted on a node's left or right child must also be binary search trees.

Keys are sorted as they are added so in-order traversal will give the inserted keys in order. A binary search tree can be /flattened/ into an array by appending the flattened right subtree to the list containing the flattened left subtree and the value of the root in that order (recursively).

#+ATTR_LATEX: :placement [H] :scale 1
#+CAPTION: A flattened binary search tree
#+NAME: fig:binary_search_tree
[[./res/small_bst.svg]]

*** Node Deletion

To delete a non-leaf node with only one subtree child, replace the node to be deleted with the root of the subtree. If a node is a leaf node (has no children) it can be removed in a single step (figure [[fig:bst_delete_0_1_children]]). Otherwise, if a node $x$ to be deleted has two children (figure [[fig:bst_delete_2_children]]):

- Identify the leftmost node $y$ in the right subtree of $x$.
- Replace the value of $x$ with the value of $y$.
- Remove the node $y$ by replacing $y$ with its right child, if it exists.

#+ATTR_LATEX: :placement [H] :scale 1
#+CAPTION: Deleting node $x$ with zero or one children
#+NAME: fig:bst_delete_0_1_children
[[./res/bst_delete_0_1_children.svg]]

#+ATTR_LATEX: :placement [H] :scale 1
#+CAPTION: Deleting node $x$ with two children
#+NAME: fig:bst_delete_2_children
[[./res/bst_delete_2_children.svg]]

*** Verifying Binary Search Trees

There are many approaches to verify a tree is a binary search tree. The most simple (but computationally complex) is to traverse the left subtree and ensure all values are lower than the current node and then traverse the right subtree, ensuring all values are larger than the value of the root and then recursively check this is true of every subtree. The same procedure can be achieved in one traversal by setting lower and upper limits $l$ and $u$ to some special extreme value and then, taking the root of the tree:

2. If the current node is empty, return /true/.
3. If the current node is not empty and is not in $(l, u)$ return /false/.
4. Else:
   1. Setting the current node's value as $u$ and using the existing value of $l$, check the left subtree is a binary search tree. If this is not true, return /false/.
   2. Setting the current node's value as $l$ and using the existing value of $u$, check the right subtree is a binary search tree. If this is not true, return /false/.
   3. Return /true/.

*** Complexity

The /balance/ at any node is the difference in height between left and right subtrees. Insert and search operations on a search tree are faster if the tree is balanced. Ideally, the median key is inserted first so that roughly half the keys are inserted to the left and half to the right, assuming the order of insertion is random. In the worst case, keys are inserted roughly in order, making the tree resemble a unary linked list. These two cases are compared in figure [[fig:bst_cases]]. Search, insert and delete on a binary search tree with $n$ nodes are $O(\log n)$ in the average case and $O(n)$ in the worst case.

#+ATTR_LATEX: :placement [H] :scale 1
#+CAPTION: Best and wort cases for a binary search tree
#+NAME: fig:bst_cases
[[./res/bst_cases.svg]]

** B Trees
** Binary Heap Trees
** Binomial Trees
** Binomial Heaps
* Dictionaries
* Graphs
